{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"tau/commonsense_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166d06c6c8b9410eac584b186f1d3bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chowder/.local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/chowder/.local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/chowder/.local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/chowder/.local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8ffc56614245cca7d7a324b3d9aac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=base_model, max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genTest = \"The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change? (A) ignore (B) enforce (C) authoritarian (D) yell at (E) avoid. Please select an answer.\"\n",
    "\n",
    "#result = pipe(genTest)\n",
    "\n",
    "#print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 12102 rows\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "modelName = \"Llama-2-7b-chat-hf\"\n",
    "\n",
    "fileName = modelName + \"generatedMaxNewTokens10.pickle\"\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(fileName, 'rb') as file:\n",
    "        generatedOutputs = pickle.load(file)\n",
    "        print(\"loaded \" + str(len(generatedOutputs)) + \" rows\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found making new object\")\n",
    "    generatedOutputs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 0 new answers in 0.06681013107299805\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "answerPattern = r'[A-E]\\)'\n",
    "\n",
    "lastSave = time.time()\n",
    "newAnswers = 0\n",
    "for q in dataset[\"test\"]:#\"validation\"\n",
    "    if q[\"question\"] not in generatedOutputs:\n",
    "        prompt = q[\"question\"]\n",
    "        for i in range(len(q[\"choices\"][\"text\"])):\n",
    "            prompt += \" (\" + q[\"choices\"][\"label\"][i] + \") \" + q[\"choices\"][\"text\"][i]\n",
    "        prompt += \".\"\n",
    "        #print(newAnswers % 10)\n",
    "        print(\"generating result for the prompt: \" + prompt)\n",
    "        result = pipe(prompt)\n",
    "        #print(result[0]['generated_text'])\n",
    "        #print(len(result[0]['generated_text']))\n",
    "        #print(len(prompt))\n",
    "        #print(len(result) - len(prompt))\n",
    "        genText = result[0]['generated_text'][len(prompt):]\n",
    "        print(\"result is: \" + genText)\n",
    "\n",
    "        print(\"correct answer is \" + q[\"answerKey\"])\n",
    "\n",
    "        potentialMatches = re.findall(answerPattern, genText)\n",
    "        print(potentialMatches)\n",
    "        if  len(potentialMatches) != 0:\n",
    "            parsedAnswer = re.findall(answerPattern, genText)[0][0]\n",
    "            print(\"parsed answer is \" + parsedAnswer)\n",
    "            generatedOutputs[q[\"question\"]] = {\"prompt\" : prompt, \"genText\" : genText, \"parsed\" : parsedAnswer}\n",
    "            newAnswers += 1\n",
    "        else:\n",
    "            #enter some infor indicating that it wasn't found\n",
    "            generatedOutputs[q[\"question\"]] = {\"prompt\" : prompt, \"genText\" : genText, \"parsed\" : \"None\"}\n",
    "            print(\"not able to parse an answer###################################################################################################################################\")\n",
    "        print(\"Number of new answers: \" + str(newAnswers))\n",
    "    if newAnswers % 10 == 1:\n",
    "        with open(fileName, 'wb') as file:\n",
    "            # Serialize and save the object to the file\n",
    "            print(\"saved \" + str(newAnswers) + \" new answers in \" + str(time.time() - lastSave))\n",
    "            pickle.dump(generatedOutputs, file)\n",
    "        lastSave = time.time()\n",
    "\n",
    "\n",
    "with open(fileName, 'wb') as file:\n",
    "    # Serialize and save the object to the file\n",
    "    print(\"saved \" + str(newAnswers) + \" new answers in \" + str(time.time() - lastSave))\n",
    "    pickle.dump(generatedOutputs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "testText = \"asdf\\n\\n\\nThe Answer is (E) Because I think it is not (A).\"\n",
    "\n",
    "answerPattern = r'\\([A-E]\\)'\n",
    "\n",
    "\n",
    "print(re.findall(answerPattern, testText)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
